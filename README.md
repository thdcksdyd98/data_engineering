# 🧠 Data Engineering Portfolio

Welcome to my data engineering portfolio — a collection of end-to-end projects designed to demonstrate my ability to build robust, scalable, and production-ready data pipelines using modern data stack tools.

This repository is intended to showcase my technical proficiency, architectural thinking, and practical experience with batch and real-time data workflows. All projects use synthetic (generated) data to simulate real-world business scenarios.

---

### ⚙️ Tech Stack
- **Workflow Orchestration**: Apache Airflow  
- **Distributed Processing**: Apache Spark (PySpark)  
- **Real-Time Streaming**: Apache Kafka  
- **Data Transformation & Modeling**: dbt  
- **Cloud Data Warehousing**: Snowflake  
- **Relational Databases**: PostgreSQL  

---

### 🚀 What You'll Find Here
- ✅ End-to-end **ELT pipelines** simulating real-world data flow from ingestion to transformation  
- ✅ Real-time streaming pipelines with **Kafka + Spark Structured Streaming**  
- ✅ Modular, testable, and production-like pipeline designs  
- ✅ Scalable workflows orchestrated with **Apache Airflow**  
- ✅ SQL and Python-based **data modeling and transformations** using dbt  

---

### 📌 Purpose

This portfolio was created with the goal of transitioning into a **Data Engineer role**. Every project here reflects my focus on building pipelines that are not just technically sound, but also **scalable, maintainable, and relevant to modern data engineering practices**.

---

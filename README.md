# ğŸ§  Data Engineering Portfolio

Welcome to my data engineering portfolio â€” a collection of end-to-end projects designed to demonstrate my ability to build robust, scalable, and production-ready data pipelines using modern data stack tools.

This repository is intended to showcase my technical proficiency, architectural thinking, and practical experience with batch and real-time data workflows. All projects use synthetic (generated) data to simulate real-world business scenarios.

---

### âš™ï¸ Tech Stack
- **Workflow Orchestration**: Apache Airflow  
- **Distributed Processing**: Apache Spark (PySpark)  
- **Real-Time Streaming**: Apache Kafka  
- **Data Transformation & Modeling**: dbt  
- **Cloud Data Warehousing**: Snowflake  
- **Relational Databases**: PostgreSQL  

---

### ğŸš€ What You'll Find Here
- âœ… End-to-end **ELT pipelines** simulating real-world data flow from ingestion to transformation  
- âœ… Real-time streaming pipelines with **Kafka + Spark Structured Streaming**  
- âœ… Modular, testable, and production-like pipeline designs  
- âœ… Scalable workflows orchestrated with **Apache Airflow**  
- âœ… SQL and Python-based **data modeling and transformations** using dbt  

---

### ğŸ“Œ Purpose

This portfolio was created with the goal of transitioning into a **Data Engineer role**. Every project here reflects my focus on building pipelines that are not just technically sound, but also **scalable, maintainable, and relevant to modern data engineering practices**.

---

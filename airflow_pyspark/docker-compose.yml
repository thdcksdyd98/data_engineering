x-airflow-common-dep: &airflow-common-dep
    postgres:
      condition: service_healthy
    airflow-init:
      condition: service_completed_successfully

x-airflow-common-vol: &airflow-common-vol
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - /var/run/docker.sock:/var/run/docker.sock

x-airflow-common-env: &airflow-common-env
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    - AIRFLOW__CORE__LOAD_EXAMPLES=False

x-airflow-common: &airflow-common
    image: apache/airflow:2.11.0
    environment: *airflow-common-env
    volumes: *airflow-common-vol
    depends_on: *airflow-common-dep

services:

  postgres:
    image: postgres:13
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./postgres_init:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always
    networks:
      - mynet

  airflow-init:
    image: apache/airflow:2.11.0
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    command: ["bash","-c","airflow db init && airflow users create --role Admin --username admin --email admin@example.com --firstname admin --lastname admin --password admin"]
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - mynet

  airflow-webserver:
    <<: *airflow-common
    user: root
    ports:
      - "8080:8080"
    command: ["bash","-c","airflow webserver"]
    networks:
      - mynet

  airflow-scheduler:
    <<: *airflow-common
    user: root
    command: ["bash","-c","airflow scheduler"]
    networks:
      - mynet

  spark-master:
    image: apache/spark:3.4.1
    command: >
      bash -c '/opt/spark/bin/spark-class org.apache.spark.deploy.master.Master --host spark-master --port 7077 --webui-port 8080'
    ports:
      - "8081:8080"
      - "7077:7077"
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:8080 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - mynet

  spark-worker:
    image: apache/spark:3.4.1
    command: >
      bash -c "/opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 --cores 2 --work-dir /opt/spark/work --webui-port 8080"
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - mynet

  data-loader:
    image: debian:bullseye-slim
    depends_on: 
      postgres:
        condition: service_healthy
    volumes:
      - ./copy_file.sh:/app/copy_file.sh
      - ./postgres_init/merged.csv:/app/merged.csv
    environment:
      - PGPASSWORD=airflow
    working_dir: /app
    entrypoint: >
      sh -c "apt-get update && apt-get install -y postgresql-client && 
      until psql -h postgres -U airflow -d airflow -c '\q'; do
        echo 'Waiting for postgres...';
        sleep 2;
      done && chmod +x copy_file.sh && ./copy_file.sh"
    networks:
      - mynet

  spark-app:
    build: ./spark_app
    depends_on:
      postgres: 
        condition: service_healthy
      spark-master:
        condition: service_healthy
    networks:
      - mynet

volumes:
  pgdata:
networks:
  mynet:
    driver: bridge
